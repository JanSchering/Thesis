{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import torch as t\n",
    "from functorch import vjp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(a,b,c):\n",
    "    return a * b + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = t.tensor(2.)\n",
    "b = t.tensor(4.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fn = partial(test, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fn(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(9.),\n",
       " <function functorch._src.eager_transforms._vjp_with_argnums.<locals>.wrapper(cotangents, retain_graph=True, create_graph=None)>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vjp(test_fn, t.tensor(1.))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward AD testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.autograd.forward_ad as fwAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fn(w, b, x):\n",
    "    print(f\"w: {w}, x: {x}, b: {b}\")\n",
    "    return w*x+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = t.tensor(2.)\n",
    "primal = w.clone().requires_grad_()\n",
    "bias = t.tensor(1.)\n",
    "tangent = t.tensor(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: 2.0, x: 10.0, b: 1.0\n",
      "UnpackedDualTensor(primal=tensor(21., grad_fn=<Identity>), tangent=tensor(10., grad_fn=<AddBackward0>))\n",
      "w: 2.0, x: 21.0, b: 1.0\n",
      "UnpackedDualTensor(primal=tensor(43., grad_fn=<Identity>), tangent=tensor(41., grad_fn=<AddBackward0>))\n"
     ]
    }
   ],
   "source": [
    "with fwAD.dual_level():\n",
    "    dual_input = fwAD.make_dual(primal, tangent)\n",
    "    # Tensors that do not not have an associated tangent are automatically\n",
    "    # considered to have a zero-filled tangent of the same shape.\n",
    "    dual_output = test_fn(dual_input, bias, t.tensor(10.))\n",
    "    fn1_results = fwAD.unpack_dual(dual_output)\n",
    "    print(fn1_results)\n",
    "    dual_output2 = test_fn(dual_input, bias, dual_output)\n",
    "    fn2_results = fwAD.unpack_dual(dual_output2)\n",
    "    print(fn2_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., requires_grad=True) tensor(10.) tensor(1.)\n",
      "tensor(2., requires_grad=True) tensor(21., grad_fn=<AddBackward0>) tensor(1.)\n",
      "func output: 43.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(41.),)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.requires_grad_()\n",
    "a = test_fn(w, bias, t.tensor(10.))\n",
    "b = test_fn(w, bias, a)\n",
    "print(f'func output: {b}')\n",
    "t.autograd.grad(b, w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experimenting with Reaction function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import uniform, normal\n",
    "from reaction import rho\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chop_and_shuffle_data(sequence, shuffle=True):\n",
    "    \"\"\"\n",
    "    Chop the training data into a set of state transitions and shuffle the resulting set.\n",
    "\n",
    "    sequences (np.ndarray): matrix of shape (n_sequences, steps_per_seq, grid_height, grid_width)\n",
    "    \"\"\"\n",
    "    steps_per_seq, _, grid_height, grid_width = sequence.shape\n",
    "    # each transition consists of 2 states\n",
    "    indexer = np.arange(2)[None, :] + np.arange(steps_per_seq - 1)[:, None]\n",
    "    chopped_set = np.zeros(\n",
    "        [(steps_per_seq - 1), 2, 2, grid_height, grid_width]\n",
    "    )\n",
    "    chopped_set = sequence.detach().numpy()[indexer]\n",
    "    if shuffle:\n",
    "        np.random.shuffle(chopped_set)\n",
    "    return t.tensor(chopped_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define probability functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p1(cells: t.Tensor, N: int, gamma: float, k1: float) -> t.Tensor:\n",
    "    n = cells[0]\n",
    "    k1_bar = k1 / ((N - 1) * (N - 2))\n",
    "    return gamma * k1_bar * n * (n - 1) * (N - n)\n",
    "\n",
    "\n",
    "def p2(cells: t.Tensor, N: int, gamma: float, k1_star: float) -> t.Tensor:\n",
    "    n = cells[0]\n",
    "    k1_star_bar = k1_star / ((N - 1) * (N - 2))\n",
    "    return gamma * k1_star_bar * n * (N - n) * (N - 1 - n)\n",
    "\n",
    "\n",
    "def p3(cells: t.Tensor, N: int, gamma: float, k2: float) -> t.Tensor:\n",
    "    n = cells[0]\n",
    "    m = cells[1]\n",
    "    k2_bar = k2 / N\n",
    "    return gamma * k2_bar * (N - n) * m\n",
    "\n",
    "\n",
    "def p4(cells: t.Tensor, N: int, gamma: float, k2_star: float) -> t.Tensor:\n",
    "    n = cells[0]\n",
    "    m = cells[1]\n",
    "    k2_star_bar = k2_star / N\n",
    "    return gamma * k2_star_bar * n * (N - m)\n",
    "\n",
    "\n",
    "def p5(cells: t.Tensor, N: int, gamma: float, k3: float) -> t.Tensor:\n",
    "    n = cells[0]\n",
    "    m = cells[1]\n",
    "    k3_bar = k3 / N\n",
    "    return gamma * k3_bar * (N - n) * (N - m)\n",
    "\n",
    "\n",
    "def p6(cells: t.Tensor, N: int, gamma: float, k3_star: float) -> t.Tensor:\n",
    "    n = cells[0]\n",
    "    m = cells[1]\n",
    "    k3_star_bar = k3_star / N\n",
    "    return gamma * k3_star_bar * n * m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13050.)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = t.zeros((2,15,15))\n",
    "grid[:] = 50\n",
    "grid[0,12:17] = 90\n",
    "N = 100 \n",
    "gamma = 0.005 \n",
    "rate_coefficients = t.tensor([0.98,0.98,0.1,0.1,0.2,0.2])\n",
    "probability_funcs = [p1,p2,p3,p4,p5,p6]\n",
    "num_steps = 1_000\n",
    "t.sum(grid[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/root/anaconda3/lib/python3.9/site-packages/torch/distributions/distribution.py:167: UserWarning: sample_n will be deprecated. Use .sample((n,)) instead\n",
      "  warnings.warn('sample_n will be deprecated. Use .sample((n,)) instead', UserWarning)\n",
      "100%|██████████| 1000/1000 [00:02<00:00, 357.71it/s]\n"
     ]
    }
   ],
   "source": [
    "grid = grid.float()\n",
    "sequence = t.zeros((num_steps, *grid.shape))\n",
    "\n",
    "for i in tqdm(range(num_steps)):\n",
    "    sequence[i] = grid.detach().clone()\n",
    "    grid = rho(grid, N, gamma, rate_coefficients, probability_funcs, num_reaction_channels=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = chop_and_shuffle_data(sequence=sequence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[:,0]\n",
    "Y_obs = dataset[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 999, 15, 15])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.permute(1,0,2,3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4363)\n"
     ]
    }
   ],
   "source": [
    "print(normal.Normal(0.5,0.1).sample((10,))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STEFunction(t.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return (input > 0).float()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output\n",
    "\n",
    "    @staticmethod\n",
    "    def jvp(ctx, grad_output):\n",
    "        return grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(X, Y):\n",
    "    return t.mean(t.sum((X-Y)**2, dim=((1,2,3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(X, D1, D2):\n",
    "    mse_D1 = MSE(X, D1)\n",
    "    #print(mse_D1)\n",
    "    mse_D2 = MSE(X, D2)\n",
    "    #print(mse_D2)\n",
    "    return (mse_D1 - mse_D2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0100, grad_fn=<SelectBackward0>, tangent=1.0)\n",
      "tensor(0.0100, grad_fn=<SelectBackward0>, tangent=0.0)\n",
      "tensor(0.0100, grad_fn=<SelectBackward0>, tangent=0.0)\n",
      "tensor(0.0100, grad_fn=<SelectBackward0>, tangent=0.0)\n",
      "tensor(0.0100, grad_fn=<SelectBackward0>, tangent=0.0)\n",
      "tensor(0.0100, grad_fn=<SelectBackward0>, tangent=0.0)\n"
     ]
    }
   ],
   "source": [
    "rate_coefficients = t.tensor([0.01,0.98,0.1,0.1,0.2,0.2])\n",
    "primal = rate_coefficients.clone().requires_grad_()\n",
    "tangents = t.eye(6)\n",
    "with fwAD.dual_level():\n",
    "    batch_size, grids_per_el, height, width = X.shape\n",
    "    channel_matrix = t.randint(high=6, size=(batch_size, height, width))\n",
    "    for tangent in tangents:\n",
    "        dual_input = fwAD.make_dual(primal, tangent)\n",
    "\n",
    "        # move the batch dimension in to match the masking\n",
    "        Y_sim = X.clone()\n",
    "        Y_sim = Y_sim.permute(1, 0, 2, 3)\n",
    "\n",
    "        print()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32804/1347480800.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mprimal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtangent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfwAD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[0;31m# See gh-54457\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iteration over a 0-d tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             warnings.warn(\n",
      "\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d tensor"
     ]
    }
   ],
   "source": [
    "#rate_coefficients = normal.Normal(0.5,0.1).sample((6,))\n",
    "rate_coefficients = t.tensor([0.01,0.98,0.1,0.1,0.2,0.2])\n",
    "\n",
    "primal0 = rate_coefficients[0].clone().requires_grad_()\n",
    "tangent0 = t.tensor(1.)\n",
    "primal1 = rate_coefficients[1].clone().requires_grad_()\n",
    "tangent1 = t.tensor(1.)\n",
    "primal2 = rate_coefficients[2].clone().requires_grad_()\n",
    "tangent2 = t.tensor(1.)\n",
    "primal3 = rate_coefficients[3].clone().requires_grad_()\n",
    "tangent3 = t.tensor(1.)\n",
    "primal4 = rate_coefficients[4].clone().requires_grad_()\n",
    "tangent4 = t.tensor(1.)\n",
    "primal5 = rate_coefficients[5].clone().requires_grad_()\n",
    "tangent5 = t.tensor(1.)\n",
    "\n",
    "params = [\n",
    "    (primal0,tangent0),\n",
    "    (primal1,tangent1),\n",
    "    (primal2,tangent2),\n",
    "    (primal3,tangent3),\n",
    "    (primal4,tangent4),\n",
    "    (primal5,tangent5)\n",
    "]\n",
    "\n",
    "Y_sim = X.clone()\n",
    "\n",
    "fwd_jacobian = []\n",
    "\n",
    "for idx, param in params:\n",
    "    primal, tangent = param\n",
    "    with fwAD.dual_level():\n",
    "\n",
    "        batch_size, grids_per_el, height, width = X.shape\n",
    "        channel_matrix = t.randint(high=6, size=(batch_size, height, width))\n",
    "\n",
    "        dual_input = fwAD.make_dual(primal, tangent)\n",
    "\n",
    "        # move the batch dimension in to match the masking\n",
    "        Y_sim = Y_sim.permute(1, 0, 2, 3)\n",
    "\n",
    "        for channel_idx in range(6):\n",
    "            channel_mask = channel_matrix == channel_idx\n",
    "            if channel_idx == 0:\n",
    "                if idx == 0:\n",
    "                    reaction_prob = p1(Y_sim[:, channel_mask], N, gamma, dual_input)\n",
    "                else:\n",
    "                    reaction_prob = p1(Y_sim[:, channel_mask], N, gamma, rate_coefficients[0])\n",
    "            if channel_idx == 1:\n",
    "                if idx == 1:\n",
    "                    reaction_prob = p1(Y_sim[:, channel_mask], N, gamma, dual_input)\n",
    "                else:\n",
    "                    reaction_prob = p1(Y_sim[:, channel_mask], N, gamma, rate_coefficients[1])\n",
    "            if channel_idx == 2:\n",
    "                if idx == 2:\n",
    "                    reaction_prob = p1(Y_sim[:, channel_mask], N, gamma, dual_input)\n",
    "                else:\n",
    "                    reaction_prob = p1(Y_sim[:, channel_mask], N, gamma, rate_coefficients[2])\n",
    "            if channel_idx == 3:\n",
    "                if idx == 0:\n",
    "                    reaction_prob = p1(Y_sim[:, channel_mask], N, gamma, dual_input)\n",
    "                else:\n",
    "                    reaction_prob = p1(Y_sim[:, channel_mask], N, gamma, rate_coefficients[3])\n",
    "            if channel_idx == 4:  \n",
    "                if idx == 0:\n",
    "                    reaction_prob = p1(Y_sim[:, channel_mask], N, gamma, dual_input)\n",
    "                else:\n",
    "                    reaction_prob = p1(Y_sim[:, channel_mask], N, gamma, rate_coefficients[4])\n",
    "            if channel_idx == 5:\n",
    "                if idx == 0:\n",
    "                    reaction_prob = p1(Y_sim[:, channel_mask], N, gamma, dual_input)\n",
    "                else:\n",
    "                    reaction_prob = p1(Y_sim[:, channel_mask], N, gamma, rate_coefficients[5])\n",
    "\n",
    "            # randomly sample a threshold value for each cell to compare the prob. against\n",
    "            num_cells = Y_sim[:, channel_mask].shape[-1]\n",
    "            #print(num_cells)\n",
    "            thresholds = uniform.Uniform(0, 1).sample((num_cells,))\n",
    "\n",
    "            dual_output2 = STEFunction.apply(dual_output - thresholds)\n",
    "            #print(dual_output2)\n",
    "\n",
    "            dual_output3 = Y_sim[:, channel_mask] + dual_output2\n",
    "            #print(dual_output3)\n",
    "\n",
    "            Y_sim[:, channel_mask] = dual_output3\n",
    "            #print(Y_sim[:, channel_mask][0,0].shape)\n",
    "\n",
    "        Y_sim = Y_sim.permute(1, 0, 2, 3)\n",
    "        dual_output_4 = MSE(X, Y_sim)\n",
    "        #print(dual_output_4)\n",
    "\n",
    "        dual_output5 = dist(X, Y_obs, Y_sim)\n",
    "        print(dual_output5)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61a57a4b5406d2de388e2f91097d4e4bcd7d5f4a46f53a795aa28a02eed27fc5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
