from typing import Callable
import torch as t
import numpy as np
from tqdm import tqdm
import random
from torch.distributions import normal
from helpers import gaussian_pdf
import matplotlib.pyplot as plt


def ABC_MCMC(
    X: t.Tensor,
    Y_obs: t.Tensor,
    theta: t.Tensor,
    model: Callable,
    distance_func: Callable,
    calc_acceptance_rate: Callable,
    epsilon: float,
    N: int,
    q: Callable,
):
    # List of all accepted theta values
    thetas = [theta]
    # History of all thetas
    theta_hist = [theta]
    # Track how many steps are necessary to generate an accepted sample
    steps = 1
    # History of necessary steps per sample
    step_hist = []
    # run the MCMC algorithm for N steps
    for _ in tqdm(range(N)):
        # generate a proposal theta* ~ q(theta*|theta) from proposal density q
        theta_star = q(theta)
        # use theta* to generate a dataset
        Y_sim = model(X, theta_star)
        # calculate the distance between the observed dataset and the dataset generated by the model using theta*
        dist = distance_func(X, Y_sim, Y_obs)
        # calculate the (Metropolis-Hastings) acceptance rate for theta*
        alpha = calc_acceptance_rate(theta_star, theta, epsilon, dist)
        # With probability <alpha>, accept theta*
        if random.random() <= alpha:
            # theta_i+1 = theta*
            theta = theta_star
            # collect the sample
            thetas.append(theta)
            # track the number of steps
            step_hist.append(steps)
            steps = 1
        else:
            steps += 1
        theta_hist.append(theta)
    return thetas, theta_hist, step_hist


if __name__ == "__main__":
    sigma = t.tensor(0.15)
    epsilon = 0.025
    theta_init = t.tensor(0.0)
    N = 10_000

    def q(theta: t.Tensor):
        return normal.Normal(theta, sigma).sample()

    def gauss_likelihood(x: t.Tensor, y: t.Tensor) -> float:
        pdf = gaussian_pdf(y, t.tensor(sigma**2))
        return pdf(x)

    def calc_distance(X: t.Tensor, Y_sim: t.Tensor, Y_obs: t.Tensor) -> float:
        if random.random() > 0.5:
            return t.abs(t.mean(Y_sim))
        else:
            return t.abs(Y_sim[0])

    def uniform_likelihood(low: float, high: float, x: t.Tensor) -> float:
        if x > high or x < low:
            return 0
        else:
            return 1 / (high - low)

    def calc_alpha(
        theta_star: t.Tensor, theta: t.Tensor, epsilon: float, dist: t.Tensor
    ) -> float:
        if dist <= epsilon and uniform_likelihood(-10, 10, theta_star) > 0:
            ratio = gauss_likelihood(theta, theta_star) / gauss_likelihood(
                theta_star, theta
            )
            return t.min(t.tensor([t.tensor(1.0), ratio]))
        else:
            return 0

    # according to the paper, the dataset should be 100 samples drawn from N(theta,1)
    def model(X, theta):
        n = normal.Normal(theta, 1)
        return n.sample_n(100)

    thetas, theta_hist, step_hist = ABC_MCMC(
        None, None, theta_init, model, calc_distance, calc_alpha, epsilon, N, q
    )

    print(f"accepted {len(thetas)} samples")

    # Visualize the results and compare to the expected posterior
    pdf_1 = gaussian_pdf(t.tensor(0), t.tensor(1 / 100))
    pdf_2 = gaussian_pdf(t.tensor(0), t.tensor(1))

    def posterior(theta: float) -> float:
        return (1 / 2) * pdf_1(theta) + (1 / 2) * pdf_2(theta)

    test_vals = t.linspace(-3, 3, 100)

    fig, axs = plt.subplots(2)
    axs[0].hist(thetas, density=True, bins=30)
    axs[0].plot(test_vals, [posterior(test_val) for test_val in test_vals], color="red")
    axs[1].plot(np.arange(len(thetas)), thetas)
    axs.flat[0].set_xlim(-4, 4)
    plt.show()
